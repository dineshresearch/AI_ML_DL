{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.datasets import imdb\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "\n",
    "\n",
    "traindata = pd.read_csv('dataset/traindata.csv', header=None)\n",
    "testdata = pd.read_csv('dataset/testdata.csv', header=None)\n",
    "\n",
    "\n",
    "X = traindata.iloc[:,1:15002]\n",
    "Y = traindata.iloc[:,0]\n",
    "C = testdata.iloc[:,0]\n",
    "T = testdata.iloc[:,1:15002]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_train = np.array(Y)\n",
    "y_test = np.array(C)\n",
    "\n",
    "X_train = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "X_test = np.reshape(testT, (testT.shape[0], 1, testT.shape[1]))\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# 1. define the network\n",
    "model = Sequential()\n",
    "model.add(GRU(32,input_dim=15000))  # try using a GRU instead, for fun\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=\"grulayer/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='loss')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1000, callbacks=[checkpointer])\n",
    "model.save(\"grulayer/grulayer_model.hdf5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.datasets import imdb\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "\n",
    "\n",
    "testdata = pd.read_csv('dataset/testdata.csv', header=None)\n",
    "\n",
    "\n",
    "C = testdata.iloc[:,0]\n",
    "T = testdata.iloc[:,1:15002]\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_test = np.array(C)\n",
    "\n",
    "\n",
    "X_test = np.reshape(testT, (testT.shape[0], 1, testT.shape[1]))\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# 1. define the network\n",
    "model = Sequential()\n",
    "model.add(GRU(32,input_dim=15000))  # try using a GRU instead, for fun\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "model.load_weights(\"grulayer/checkpoint-07.hdf5\")\n",
    "y_pred = model.predict_classes(X_test)\n",
    "print(y_pred)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
    "y_prob = model.predict_proba(X_test)\n",
    "np.savetxt(\"gru.txt\", y_prob)\n",
    "\n",
    "\n",
    "import os\n",
    "for file in os.listdir(\"grulayer/\"):\n",
    "  model.load_weights(\"grulayer/\"+file)\n",
    "  y_pred = model.predict_classes(X_test)\n",
    "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "  loss, accuracy = model.evaluate(X_test, y_test)\n",
    "  print(file)\n",
    "  print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
    "  print(\"---------------------------------------------------------------------------------\")\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  recall = recall_score(y_test, y_pred , average=\"binary\")\n",
    "  precision = precision_score(y_test, y_pred , average=\"binary\")\n",
    "  f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
    "\n",
    "  print(\"accuracy\")\n",
    "  print(\"%.3f\" %accuracy)\n",
    "  print(\"precision\")\n",
    "  print(\"%.3f\" %precision)\n",
    "  print(\"recall\")\n",
    "  print(\"%.3f\" %recall)\n",
    "  print(\"f1score\")\n",
    "  print(\"%.3f\" %f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.datasets import imdb\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "\n",
    "\n",
    "traindata = pd.read_csv('dataset/traindata.csv', header=None)\n",
    "testdata = pd.read_csv('dataset/testdata.csv', header=None)\n",
    "\n",
    "\n",
    "X = traindata.iloc[:,1:15002]\n",
    "Y = traindata.iloc[:,0]\n",
    "C = testdata.iloc[:,0]\n",
    "T = testdata.iloc[:,1:15002]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_train = np.array(Y)\n",
    "y_test = np.array(C)\n",
    "\n",
    "X_train = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "X_test = np.reshape(testT, (testT.shape[0], 1, testT.shape[1]))\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# 1. define the network\n",
    "model = Sequential()\n",
    "model.add(LSTM(32,input_dim=15000))  # try using a GRU instead, for fun\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=\"lstmlayer/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='loss')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1000, callbacks=[checkpointer])\n",
    "model.save(\"lstmlayer/lstmlayer_model.hdf5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.datasets import imdb\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "\n",
    "\n",
    "testdata = pd.read_csv('dataset/testdata.csv', header=None)\n",
    "\n",
    "\n",
    "C = testdata.iloc[:,0]\n",
    "T = testdata.iloc[:,1:15002]\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_test = np.array(C)\n",
    "\n",
    "\n",
    "X_test = np.reshape(testT, (testT.shape[0], 1, testT.shape[1]))\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "# 1. define the network\n",
    "model = Sequential()\n",
    "model.add(LSTM(32,input_dim=15000))  # try using a GRU instead, for fun\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "model.load_weights(\"lstmlayer/checkpoint-07.hdf5\")\n",
    "y_pred = model.predict_classes(X_test)\n",
    "print(y_pred)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
    "y_prob = model.predict_proba(X_test)\n",
    "np.savetxt(\"gru.txt\", y_prob)\n",
    "\n",
    "\n",
    "import os\n",
    "for file in os.listdir(\"lstmlayer/\"):\n",
    "  model.load_weights(\"lstmlayer/\"+file)\n",
    "  y_pred = model.predict_classes(X_test)\n",
    "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "  loss, accuracy = model.evaluate(X_test, y_test)\n",
    "  print(file)\n",
    "  print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
    "  print(\"---------------------------------------------------------------------------------\")\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  recall = recall_score(y_test, y_pred , average=\"binary\")\n",
    "  precision = precision_score(y_test, y_pred , average=\"binary\")\n",
    "  f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
    "\n",
    "  print(\"accuracy\")\n",
    "  print(\"%.3f\" %accuracy)\n",
    "  print(\"precision\")\n",
    "  print(\"%.3f\" %precision)\n",
    "  print(\"recall\")\n",
    "  print(\"%.3f\" %recall)\n",
    "  print(\"f1score\")\n",
    "  print(\"%.3f\" %f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.datasets import imdb\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "traindata = pd.read_csv('dataset/traindata.csv', header=None)\n",
    "testdata = pd.read_csv('dataset/testdata.csv', header=None)\n",
    "\n",
    "\n",
    "X = traindata.iloc[:,1:15002]\n",
    "Y = traindata.iloc[:,0]\n",
    "C = testdata.iloc[:,0]\n",
    "T = testdata.iloc[:,1:15002]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_train = np.array(Y)\n",
    "y_test = np.array(C)\n",
    "\n",
    "X_train = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "X_test = np.reshape(testT, (testT.shape[0], 1, testT.shape[1]))\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# 1. define the network\n",
    "def create_model():\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(32,input_dim=15000))  # try using a GRU instead, for fun\n",
    "  model.add(Dropout(0.1))\n",
    "  model.add(Dense(1))\n",
    "  model.add(Activation('sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, nb_epoch=1000, batch_size=4, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(model, X_train, y_train, cv=kfold)\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.datasets import imdb\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "\n",
    "\n",
    "traindata = pd.read_csv('dataset/traindata.csv', header=None)\n",
    "testdata = pd.read_csv('dataset/testdata.csv', header=None)\n",
    "\n",
    "\n",
    "X = traindata.iloc[:,1:15002]\n",
    "Y = traindata.iloc[:,0]\n",
    "C = testdata.iloc[:,0]\n",
    "T = testdata.iloc[:,1:15002]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_train = np.array(Y)\n",
    "y_test = np.array(C)\n",
    "\n",
    "X_train = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))#ROW,1,COL TENSORCONVERSION\n",
    "X_test = np.reshape(testT, (testT.shape[0], 1, testT.shape[1]))\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "# 1. define the network\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(64,input_dim=15000))  # try using a GRU instead, for fun\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=\"rnnlayer/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='loss')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1000, callbacks=[checkpointer])\n",
    "model.save(\"rnnlayer/rnnlayer_model.hdf5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.datasets import imdb\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import (precision_score, recall_score,\n",
    "                             f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "\n",
    "\n",
    "testdata = pd.read_csv('dataset/testdata.csv', header=None)\n",
    "\n",
    "\n",
    "C = testdata.iloc[:,0]\n",
    "T = testdata.iloc[:,1:15002]\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_test = np.array(C)\n",
    "\n",
    "\n",
    "X_test = np.reshape(testT, (testT.shape[0], 1, testT.shape[1]))\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "# 1. define the network\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(64,input_dim=15000))  # try using a GRU instead, for fun\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "import os\n",
    "for file in os.listdir(\"rnnlayer/\"):\n",
    "  model.load_weights(\"rnnlayer/\"+file)\n",
    "  y_pred = model.predict_classes(X_test)\n",
    "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "  loss, accuracy = model.evaluate(X_test, y_test)\n",
    "  print(file)\n",
    "  print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
    "  print(\"---------------------------------------------------------------------------------\")\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  recall = recall_score(y_test, y_pred , average=\"binary\")\n",
    "  precision = precision_score(y_test, y_pred , average=\"binary\")\n",
    "  f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
    "\n",
    "  print(\"accuracy\")\n",
    "  print(\"%.3f\" %accuracy)\n",
    "  print(\"precision\")\n",
    "  print(\"%.3f\" %precision)\n",
    "  print(\"recall\")\n",
    "  print(\"%.3f\" %recall)\n",
    "  print(\"f1score\")\n",
    "  print(\"%.3f\" %f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMDLINE PARSING + DEFINE FUNCTIONS FOR GRU,LSTM,RNN****+++hyperparameter 10fold cv for dnn +++ cml algo-->2.confusionmtx with metrics equation userdefined,1.roccurve,look on dnn all 7 program ,try various normalization,add visualization to df with seaborn or matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--sum] N [N ...]\n",
      "ipykernel_launcher.py: error: argument N: invalid int value: '/run/user/1001/jupyter/kernel-f79e16db-34dd-4657-b557-2718be35bb6d.json'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "parser.add_argument('integers', metavar='N', type=int, nargs='+',\n",
    "                    help='an integer for the accumulator')\n",
    "parser.add_argument('--sum', dest='accumulate', action='store_const',\n",
    "                    const=sum, default=max,\n",
    "                    help='sum the integers (default: find the max)')\n",
    "\n",
    "args = parser.parse_args()\n",
    "print args.accumulate(args.integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
